---
layout: post
title: Quality Assurance
tags: []
status: pending
type: post
published: false
meta:
  _edit_last: '4'
---
<img style="float: right; padding: 3px 0px 3px 10px;" title="Quality Assurance" src="http://blogs.headspring.com/content/binary/QualityAssurance.gif" alt="Quality Assurance" />

One of the activities that is always involved in software is quality assurance. What
does that mean to Headspring and is that a focus that Headspring would ever cut time
from to ensure a timely delivery?

First and foremost, quality should be supreme and prioritized above quantity. Here
at <a href="http://www.headspring.com/" target="_blank">Headspring</a> we are all
about quality, even to the point where every project comes with a comprehensive warranty
that is included with a project's price from start to finish and after an initial
project finishes we offer an additional warranty that may continue year over year
warranting against any software failure that is encountered as a result of a defect.
(We will fix defects at no charge.)

During projects, we don't charge by the hour our rent talent. We only charge from
results. Our clients need software of a certain quality capacity. We are going to
deliver results. If we mess up and there are quite a few defects and the software
fails, per our warranties, it is our problem and we will resolve it. We're selling
end results hands down. That's it. The way we're structured as a business centers
in quality and requires quality in order to operate.
<h2>Quality does not mean testing, first and foremost.</h2>
In fact, we do not have anyone on staff with the title of "tester." Now let that sink
in a little bit. How do we have such high quality requirements, yet not have anyone
on staff with the title of "tester?"
<ul>
	<li>Every single person involved in the project has a quality responsibility. After you've
built anything, be it a house or a bridge or a car, you need to inspect the artifact
and detect flaws.</li>
	<li>Once the flaws are already there, your options for preventing them are gone. So, while
testing and inspection of the finished product is necessary (just as one would inspect
a car coming off of an assembly line or a house after construction), we do put most
of our efforts into preventing problems up front.
<ul>
	<li>We have certain checkpoints that test, not only the finished and working software,
but also we test the specifications that go into producing that software and moreover
the analysis that is meant to produce that specification. We test the business goals
that are set forth that will give birth to the analysis that then drives the rest
of the process.</li>
	<li>Our quality assurance process brings many inspections as far forward in the process
as possible so that the final inspection of the working software is diminished because
we've caught so many issues so much further up the line. (Preventive protection is
wise.)
<img style="padding-top: 3px;" title="Preventive Protection (thinking ahead to avoid potential pain)" src="http://blogs.headspring.com/content/binary/PreventiveProtection.jpg" alt="Preventive Protection (thinking ahead to avoid potential pain)" /></li>
</ul>
</li>
	<li>First and foremost amongst goals is making sure that we are headed in the right direction
and going solve the business need, because if you're not going in that direction it
doesn't matter how good the code is. (It's the wrong code.)</li>
	<li>Furthermore, features have specifications for each individual function, and if your
project is like the norm, 65% of the features are rarely or never used.
<img style="padding-top: 3px; padding-bottom: 3px;" title="if your project is like the norm, 65% of the features are rarely or never used" src="http://blogs.headspring.com/content/binary/YAGNI.jpg" alt="if your project is like the norm, 65% of the features are rarely or never used" />

Schedule and budget risk may be introduced by developing too much software, so features
(before being codified) should be:
<ul>
	<li>properly specified and tested</li>
	<li>able to pass client inspection (to ensure the proper number of fields are on a screen,
that a button is going to do the right thing, etc.)</li>
	<li>vetted and tested by a subject matter expert (maybe a user/the proper party at the
client organization)</li>
</ul>
</li>
</ul>
Then the remaining gap to fill in development may be summarized by asking: "Now that
the software is working, is it exactly as the approved specification?"

<img style="float: right; padding: 10px 0px 3px 10px;" title="automated testing" src="http://blogs.headspring.com/content/binary/testing.gif" alt="automated testing" />
<h2>Unit Tests versus Full System Tests</h2>
Pervasive in the entire process is automated testing at every level of the software,
and the full system testing is the most important.

There is a fad going on within the industry with unit testing, and, yes, unit testing
is important, but full system testing is where the rubber meets the road. While all
of your unit tests may pass your software may still fail if a file format is wrong.
The system tests are going to fail when there is a problem. The unit tests help developers
narrow in on problems quickly.

We recommend automated testing in general along with frequent builds and frequent
deployments to UAT environments to be put in front of a client so that they may do
the final inspection and close the long loop that has all the checkpoints along the
way. You have to sew the process together with frequent UAT checkpoints, frequent
milestone deliveries, and lots and lots of communication.

On average we like to close the loop every two weeks. For our larger and more risky
projects we do it every week to mitigate risk. On smaller projects, interestingly
enough, it has even gone out to three weeks and four weeks. In this, we find that
there is an inverse proportion between the size of a project and the length of the
iteration.
<h2>We Want Movie Quality Up Front!</h2>
for the requirements process and automation of software. If there are no software
testers on staff, how does the automated testing happen?
<ul>
	<li>While no one holds the specific title of "Tester," every single programmer tests,
and every single programmer not only writes, but also writes tests that are going
to verify functionality.</li>
	<li>Writing a test for the first time isn't necessarily where one gains the most value
as one is making sure that the test works at that point anyways. It is a week or three
months down the road when you make a sweeping change or even a small change to another
section of the software that the value becomes obvious when the test fails alerting
you to the fact that you just broke something that used to work.
<ul>
	<li>As value of the test is realized, it saves you quite a bit of reputation with the
client as, otherwise, as soon as the client is surprised by something or something
is broken in a fresh build of the software, you'll lose credibility.</li>
	<li>We're never going to prevent all instances where we introduce problems into the software.
However, we can prevent the problems from surviving up to the point where in a customer
would find them.</li>
</ul>
</li>
</ul>
<img src="http://blogs.headspring.com/aggbug.ashx?id=f4a95789-a34d-420a-958e-229e72aa4c58" alt="" width="0" height="0" />
