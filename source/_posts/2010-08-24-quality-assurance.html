---
layout: post
title: Quality Assurance
tags: []
status: pending
type: post
published: false
meta:
  _edit_last: '4'
---
<p><img style="float: right; padding: 3px 0px 3px 10px;" title="Quality Assurance" src="http://blogs.headspring.com/content/binary/QualityAssurance.gif" alt="Quality Assurance" /></p>
<p>One of the activities that is always involved in software is quality assurance. What<br />
does that mean to Headspring and is that a focus that Headspring would ever cut time<br />
from to ensure a timely delivery?</p>
<p>First and foremost, quality should be supreme and prioritized above quantity. Here<br />
at <a href="http://www.headspring.com/" target="_blank">Headspring</a> we are all<br />
about quality, even to the point where every project comes with a comprehensive warranty<br />
that is included with a project's price from start to finish and after an initial<br />
project finishes we offer an additional warranty that may continue year over year<br />
warranting against any software failure that is encountered as a result of a defect.<br />
(We will fix defects at no charge.)</p>
<p>During projects, we don't charge by the hour our rent talent. We only charge from<br />
results. Our clients need software of a certain quality capacity. We are going to<br />
deliver results. If we mess up and there are quite a few defects and the software<br />
fails, per our warranties, it is our problem and we will resolve it. We're selling<br />
end results hands down. That's it. The way we're structured as a business centers<br />
in quality and requires quality in order to operate.</p>
<h2>Quality does not mean testing, first and foremost.</h2>
<p>In fact, we do not have anyone on staff with the title of "tester." Now let that sink<br />
in a little bit. How do we have such high quality requirements, yet not have anyone<br />
on staff with the title of "tester?"</p>
<ul>
<li>Every single person involved in the project has a quality responsibility. After you've<br />
built anything, be it a house or a bridge or a car, you need to inspect the artifact<br />
and detect flaws.</li>
<li>Once the flaws are already there, your options for preventing them are gone. So, while<br />
testing and inspection of the finished product is necessary (just as one would inspect<br />
a car coming off of an assembly line or a house after construction), we do put most<br />
of our efforts into preventing problems up front.</p>
<ul>
<li>We have certain checkpoints that test, not only the finished and working software,<br />
but also we test the specifications that go into producing that software and moreover<br />
the analysis that is meant to produce that specification. We test the business goals<br />
that are set forth that will give birth to the analysis that then drives the rest<br />
of the process.</li>
<li>Our quality assurance process brings many inspections as far forward in the process<br />
as possible so that the final inspection of the working software is diminished because<br />
we've caught so many issues so much further up the line. (Preventive protection is<br />
wise.)<br />
<img style="padding-top: 3px;" title="Preventive Protection (thinking ahead to avoid potential pain)" src="http://blogs.headspring.com/content/binary/PreventiveProtection.jpg" alt="Preventive Protection (thinking ahead to avoid potential pain)" /></li>
</ul>
</li>
<li>First and foremost amongst goals is making sure that we are headed in the right direction<br />
and going solve the business need, because if you're not going in that direction it<br />
doesn't matter how good the code is. (It's the wrong code.)</li>
<li>Furthermore, features have specifications for each individual function, and if your<br />
project is like the norm, 65% of the features are rarely or never used.<br />
<img style="padding-top: 3px; padding-bottom: 3px;" title="if your project is like the norm, 65% of the features are rarely or never used" src="http://blogs.headspring.com/content/binary/YAGNI.jpg" alt="if your project is like the norm, 65% of the features are rarely or never used" /></p>
<p>Schedule and budget risk may be introduced by developing too much software, so features<br />
(before being codified) should be:</p>
<ul>
<li>properly specified and tested</li>
<li>able to pass client inspection (to ensure the proper number of fields are on a screen,<br />
that a button is going to do the right thing, etc.)</li>
<li>vetted and tested by a subject matter expert (maybe a user/the proper party at the<br />
client organization)</li>
</ul>
</li>
</ul>
<p>Then the remaining gap to fill in development may be summarized by asking: "Now that<br />
the software is working, is it exactly as the approved specification?"</p>
<p><img style="float: right; padding: 10px 0px 3px 10px;" title="automated testing" src="http://blogs.headspring.com/content/binary/testing.gif" alt="automated testing" /></p>
<h2>Unit Tests versus Full System Tests</h2>
<p>Pervasive in the entire process is automated testing at every level of the software,<br />
and the full system testing is the most important.</p>
<p>There is a fad going on within the industry with unit testing, and, yes, unit testing<br />
is important, but full system testing is where the rubber meets the road. While all<br />
of your unit tests may pass your software may still fail if a file format is wrong.<br />
The system tests are going to fail when there is a problem. The unit tests help developers<br />
narrow in on problems quickly.</p>
<p>We recommend automated testing in general along with frequent builds and frequent<br />
deployments to UAT environments to be put in front of a client so that they may do<br />
the final inspection and close the long loop that has all the checkpoints along the<br />
way. You have to sew the process together with frequent UAT checkpoints, frequent<br />
milestone deliveries, and lots and lots of communication.</p>
<p>On average we like to close the loop every two weeks. For our larger and more risky<br />
projects we do it every week to mitigate risk. On smaller projects, interestingly<br />
enough, it has even gone out to three weeks and four weeks. In this, we find that<br />
there is an inverse proportion between the size of a project and the length of the<br />
iteration.</p>
<h2>We Want Movie Quality Up Front!</h2>
<p>for the requirements process and automation of software. If there are no software<br />
testers on staff, how does the automated testing happen?</p>
<ul>
<li>While no one holds the specific title of "Tester," every single programmer tests,<br />
and every single programmer not only writes, but also writes tests that are going<br />
to verify functionality.</li>
<li>Writing a test for the first time isn't necessarily where one gains the most value<br />
as one is making sure that the test works at that point anyways. It is a week or three<br />
months down the road when you make a sweeping change or even a small change to another<br />
section of the software that the value becomes obvious when the test fails alerting<br />
you to the fact that you just broke something that used to work.</p>
<ul>
<li>As value of the test is realized, it saves you quite a bit of reputation with the<br />
client as, otherwise, as soon as the client is surprised by something or something<br />
is broken in a fresh build of the software, you'll lose credibility.</li>
<li>We're never going to prevent all instances where we introduce problems into the software.<br />
However, we can prevent the problems from surviving up to the point where in a customer<br />
would find them.</li>
</ul>
</li>
</ul>
<p><img src="http://blogs.headspring.com/aggbug.ashx?id=f4a95789-a34d-420a-958e-229e72aa4c58" alt="" width="0" height="0" /></p>
