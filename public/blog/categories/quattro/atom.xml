<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: quattro | Headspring Labs]]></title>
  <link href="http://www.headspringlabs.com/blog/categories/quattro/atom.xml" rel="self"/>
  <link href="http://www.headspringlabs.com/"/>
  <updated>2013-09-20T17:34:09-05:00</updated>
  <id>http://www.headspringlabs.com/</id>
  <author>
    <name><![CDATA[Headspring Labs]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Powershell and Calling External Executables]]></title>
    <link href="http://www.headspringlabs.com/blog/powershell-and-calling-external-executables/"/>
    <updated>2011-08-18T00:00:00-05:00</updated>
    <id>http://www.headspringlabs.com/blog/powershell-and-calling-external-executables</id>
    <content type="html"><![CDATA[<p>In my last post on <a href="http://www.headspring.com/2011/08/avoiding-on-error-resume-next-when-using-powershell">error handling when using Powershell</a>, we saw how Powershell's default behavior for uncaught exceptions allows the rest of your script to continue running in a likely-invalid state, and how setting <strong>$global:ErrorActionPreference = "Stop"</strong> changes that behavior to stop as soon as an uncaught exception surfaces.</p>


<p>Unfortunately, we can <em>still</em> be surprised by Powershell's error handling behavior. There are two main categories of errors your Powershell script might encounter:</p>


<ol>
<li>Uncaught exceptions thrown by your Powershell process.</li>
<li>Failure exit codes returned by external programs that your Powershell process invoked.</li>
</ol>


<p>When we set the ErrorActionPreference to "Stop", we only change the behavior of uncaught exceptions. We have to work a little harder to cover the other category. Consider a Powershell script that calls out to an external program which returns its own failure exit code:</p>


<p><div><script src='https://gist.github.com/1150145.js'></script>
<noscript><pre><code></code></pre></noscript></div>
</p>


<p>External executables are invoked using similar syntax as Powershell commands like <code>write-host</code>, so it isn't obvious that there's anything special about the call to <code>schtasks</code>. Since it is an external program, however, it can't exactly throw a .NET exception - it might not even <em>be</em> a .NET program.</p>


<p>The lingua franca of command-line failure is exit codes, rather than exceptions. An exit code of 0 means "all is well", and anything else should be treated as a failure.</p>


<p>We have two problems:</p>


<ol>
<li>calls to external programs don't feel special to the reader, even though they behave differently from normal Powershell commands</li>
<li>we're tempted to explicitly follow each external call with a corresponding test-and-throw code block, which would be ugly and distracting to the reader</li>
</ol>


<p>Fortunately, we can take advantage of Powershell's flexible syntax to address both of these concerns. It turns out that a Powershell function can accept a <em>code block</em> as an argument, effectively allowing us to add new keywords to the language. Consider the helper function 'exec':</p>


<p><div><script src='https://gist.github.com/1150147.js'></script>
<noscript><pre><code></code></pre></noscript></div>
</p>


<p>When we use exec, we can pass it a { code block surrounded in braces }. This way we can make our external commands stand out as special, using just one new 'keyword', and we get the error handling we wanted to boot:</p>


<p><div><script src='https://gist.github.com/1150152.js'></script>
<noscript><pre><code></code></pre></noscript></div>
</p>


<p>Eureka! We can ensure that both categories of failures will stop execution dead in its tracks.</p>


<p>To sum up, the default error handling behavior in Powershell is dramatically different from that of other .NET languages. <strong>By default, your script will happily and disasterously continue running even when throwing exceptions.</strong> This can leave the user of the script confused as to whether the process has actually succeeded or failed. By setting the global ErrorActionPreference and by wrapping external commands with the 'exec' helper function, Powershell's behavior can become what we expected in the first place.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unit Testing Best Practices: Know your test's lifecycle, Part 1]]></title>
    <link href="http://www.headspringlabs.com/blog/unit-testing-best-practices-know-your-tests-lifecycle-part-1/"/>
    <updated>2011-08-17T00:00:00-05:00</updated>
    <id>http://www.headspringlabs.com/blog/unit-testing-best-practices-know-your-tests-lifecycle-part-1</id>
    <content type="html"><![CDATA[<p>A long time ago, my development team decided to write our own unit test framework. Today that would be ridiculous, because there are so many mature options from the .NET community (NUnit, MbUnit, xUnit, MsTest, to name a few). But this was 2002, .NET was brand new, and we thought attribute-based metadata was a better approach to describing tests than the interface-and-naming-convention pattern that NUnit required at the time.</p>


<p>Of course, about the same time we'd gotten our homegrown framework running and built a couple thousand tests with it, NUnit 2.0 came out with a brand-new and shiny attribute-based metadata approach. We spent the next 3 years wishing we had the time to port all our tests to NUnit, and stealing features from it for our custom test runner.</p>


<p>One of the things I really envied about NUnit was its test method lifecycle. We support setup and teardown methods, but we never went beyond that. NUnit, by comparison has a rich set of events that occur for each test that gets run. Whenever I teach our <a title="Agile Bootcamp" href="http://www.headspring.com/services/developer-training/agile-bootcamp">Agile Boot Camp class</a>, I always make sure to walk students through the finer points of a test fixture's life, so they'll know the best way to organize their test code. When you're first writing unit tests, knowing whether to put something in the test method, the setup, the fixture setup, or somewhere else, just isn't all that obvious. Oftentimes, I see people putting expensive initialization code into every single test method, which causes their tests to run slower than they need to.</p>


<p><strong>Note: </strong>I'm using NUnit for these examples. Other frameworks have different semantics, and I recommend you write some similar code for whatever library you use. For me, NUnit's always been the draft horse of .NET test frameworks - stable, steady, gets along with everybody, and does what I expect.</p>


<p>If you're not already familiar with NUnit, take a look at their <a title="NUnit Quick-start guide" href="http://www.nunit.org/index.php?p=quickStart&amp;r=2.6">quick-start guide </a>and then come back. I'll wait.</p>


<p>Now, take a look at this simple test fixture class:</p>


<p><div><script src='https://gist.github.com/1200910.js'></script>
<noscript><pre><code></code></pre></noscript></div>
</p>


<p>This fixture has two test methods, one which passes and one which fails. When we run the test framework in NUnit's test runner, we get this output:</p>


<p>&nbsp;</p>


<pre>Constructor
TestFixtureSetup
SetUp
First Test passes!
TearDown
SetUp
Second Test fails!
TearDown
TestFixtureTearDown
Dispose</pre>


<p>So, what does this tell us about the lifecycle of a test within a fixture?</p>


<ol>
<li><strong>The Test Fixture class is constructed once.</strong>  NUnit invokes all the test methods on the same fixture instance.</li>
<li><strong>TestFixtureSetup and TestFixtureTeardDown run one time. </strong>TestFixtureSetup is run immediately after construction, and only runs once for the entire test run. TestFixtureTearDown is called immediately after all test methods have been run.</li>
<li><strong>TestSetup and TestTearDown run around every test</strong>. These two methods execute immediately before, and immediately after, each test method.</li>
<li><strong>TestTearDown runs even when a test fails.</strong> Noice that Even when the second test exits with a failed assertion, the TearDown method is still called for that method.</li>
<li><strong>NUnit knows about IDisposable.</strong> Did you notice that our class implemented IDisposable? IDisposable is .NET's canonical interface for when you have a resource whose scope needs to be managed. The IDisposable.Dispose method is the last method called on our fixture class.</li>
</ol>


<div>So now that we know what interesting methods are called, when, and how often, what can we conclude? Well, here's my first guideline for you:</div>


<div>
<ol>
<li><strong>Use constructors for class-wide setup. Use IDisposable for class-wide cleanup. Avoid TestFixtureSetup and TestFixtureTearDown. </strong>I've found that the fixture-level setup and teardown methods create more confusion than they're worth. In Part 2, we'll see just how complicated this can get when you have a base class for your test fixture. But even with a simple fixture class like the above, There's no reason to have two places for your class' initialization and cleanup code. That's basically a guarantee that it's going to get chopped up between the two with no rhyme or reason. Every .NET developer knows the constructor/dispose pattern. We don't need another one.</li>
</ol>
<p>Hopefully this paints a clearer picture of what happens when you actually run a set of unit tests. Next time, we're going to see what happens when we give our test fixture a base class with its own setup, teardown, and test methods, and how they interact with the ones we already have.</p>
</div>


<p><a href="http://www.headspring.com/resources/whitepapers/">Learn more about unit tests and the Agile Difference by visiting our Whitepapers page</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Requirement Gathering Techniques | Drawing 101]]></title>
    <link href="http://www.headspringlabs.com/blog/requirement-gathering-techniques-drawing-101/"/>
    <updated>2011-07-29T00:00:00-05:00</updated>
    <id>http://www.headspringlabs.com/blog/requirement-gathering-techniques-drawing-101</id>
    <content type="html"><![CDATA[<p>If it can't be drawn, it can't be built. It’s the mantra in our office. Having heard it almost every day that I have been here at Headspring, I never really understood its power before I set out on the (thought to be exciting) task of rebuilding the company website.</p>


<p>I knew I’d have to gather requirements for the site. Having no idea of what requirement gathering techniques were out there I just started a word document list: pages, forms, content.  When I sat down with the team to start developing the site, they needed more information about site flow.</p>


<p>I tried explaining with my 17 page word document, highlighted and notes scribbled on the side, but no one was getting it (including myself). When one brilliant person suggested that we draw it out, markers opened and the drawing began: home page, top-nav pages, children pages, forms and so on.</p>


<p>When we were done the picture finally became clear. All my requirements, even those that I had forgotten about, were there and easy to follow. No matter what you’re building, please learn from my mistake and start off drawing your requirements, because if it can’t be drawn, it can’t be built.</p>

]]></content>
  </entry>
  
</feed>
